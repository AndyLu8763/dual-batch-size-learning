{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43adf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e278fc",
   "metadata": {},
   "source": [
    "### Add RandAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fb0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Setting\n",
    "device_index = -1\n",
    "MIXED_PRECISION_FLAG = False\n",
    "JIT_COMPILE_FLAG = False\n",
    "\n",
    "# Dataloader Setting\n",
    "batch_size = 512\n",
    "validation_batch_size = 1000\n",
    "drop_remainder = True\n",
    "num_parallel_calls = tf.data.AUTOTUNE\n",
    "rand_augment = True\n",
    "\n",
    "# Model Setting\n",
    "resizing = 32\n",
    "n = 3\n",
    "rate = 0.2\n",
    "classes = 100\n",
    "\n",
    "# Training Setting\n",
    "epochs = 160 # 160\n",
    "## loss function\n",
    "learning_rate = 1e-1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4 # 1e-4\n",
    "## lr scheduler\n",
    "milestones = [80, 120] # [80, 120]\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d291fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f'Numbers of Physical Devices: {len(physical_devices)}')\n",
    "tf.config.set_visible_devices(physical_devices[device_index], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[device_index], True)\n",
    "print(f'Using device: {physical_devices[device_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d768bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only TPUs support 'mixed_bfloat16'\n",
    "# if using NVIDIA GPUs, choose 'mixed_float16'\n",
    "if MIXED_PRECISION_FLAG:\n",
    "    policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "    keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f'Policy: {policy.name}')\n",
    "    print(f'Compute dtype: {policy.compute_dtype}')\n",
    "    print(f'Variable dtype: {policy.variable_dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar100(\n",
    "    batch_size: int,\n",
    "    validation_batch_size: int,\n",
    "    resizing: int = 32,\n",
    "    drop_remainder: bool = False,\n",
    "    num_parallel_calls: int = tf.data.AUTOTUNE,\n",
    "    rand_augment: bool = False\n",
    "):\n",
    "    # for cifar-10\n",
    "    ## mean = [0.491, 0.482, 0.447]\n",
    "    ## variance = [0.061, 0.059, 0.068]\n",
    "    # for cifar-100\n",
    "    ## mean = [0.507, 0.487, 0.441]\n",
    "    ## variance = [0.072, 0.066, 0.076]\n",
    "    mean = [0.507, 0.487, 0.441]\n",
    "    variance = [0.072, 0.066, 0.076]\n",
    "    def map_train_before_cache(image, resizing):\n",
    "        transform = keras.Sequential([\n",
    "            keras.layers.Resizing(height=resizing, width=resizing)\n",
    "        ])\n",
    "        return transform(image)\n",
    "    \n",
    "    def map_train_after_cache(image, rand_augment):\n",
    "        transform = tf.keras.Sequential()\n",
    "        if rand_augment:\n",
    "            transform.add(keras_cv.layers.RandAugment(\n",
    "                value_range=(0, 255),\n",
    "                augmentations_per_image=2,\n",
    "                magnitude=0.2\n",
    "            ))\n",
    "        else:\n",
    "            transform.add(keras.layers.RandomTranslation(\n",
    "                height_factor=0.125,\n",
    "                width_factor=0.125,\n",
    "                fill_mode='constant'\n",
    "            ))\n",
    "        transform.add(keras.layers.RandomFlip('horizontal'))\n",
    "        transform.add(keras.layers.Rescaling(1/255))\n",
    "        transform.add(keras.layers.Normalization(mean=mean, variance=variance))\n",
    "        return transform(image)\n",
    "    \n",
    "    def map_test(image, resizing):\n",
    "        transform = keras.Sequential([\n",
    "            keras.layers.Resizing(height=resizing, width=resizing),\n",
    "            keras.layers.Rescaling(1/255),\n",
    "            keras.layers.Normalization(mean=mean, variance=variance)\n",
    "        ])\n",
    "        return transform(image)\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "    dataloader = {\n",
    "        'train': (tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "                  .map(lambda x, y: (map_train_before_cache(x, resizing), y),\n",
    "                       num_parallel_calls=num_parallel_calls)\n",
    "                  .cache()\n",
    "                  .shuffle(buffer_size=len(x_train))\n",
    "                  .map(lambda x, y: (map_train_after_cache(x, rand_augment), y),\n",
    "                       num_parallel_calls=num_parallel_calls)\n",
    "                  .batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "                  .prefetch(buffer_size=tf.data.AUTOTUNE)),\n",
    "        'test': (tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "                 .map(lambda x, y: (map_test(x, resizing), y),\n",
    "                      num_parallel_calls=num_parallel_calls)\n",
    "                 .cache()\n",
    "                 .batch(batch_size=validation_batch_size)\n",
    "                 .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "    }\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cifar_resnet(\n",
    "    resizing: int = 32,\n",
    "    n: int = 3,\n",
    "    rate: float = 0.2,\n",
    "    classes: int = 100\n",
    ") -> keras.Model:\n",
    "    def basic_block(x: keras.Input, filters: int, conv_shortcut: bool = False):\n",
    "        if conv_shortcut:\n",
    "            shortcut = keras.layers.Conv2D(\n",
    "                filters, 1, strides=2, kernel_initializer='he_normal'\n",
    "            )(x)\n",
    "            shortcut = keras.layers.BatchNormalization(epsilon=1.001e-5)(shortcut)\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters, 3, strides=2, padding='same', kernel_initializer='he_normal'\n",
    "            )(x)\n",
    "        else:\n",
    "            shortcut = x\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters, 3, padding='same', kernel_initializer='he_normal'\n",
    "            )(x)\n",
    "        x = keras.layers.BatchNormalization(epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        x = keras.layers.Conv2D(filters, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "        x = keras.layers.BatchNormalization(epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Add()([shortcut, x])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "    def basic_stack(x: keras.Input, filters: int, conv_shortcut: bool = False):\n",
    "        for i in range(n):\n",
    "            if i == 0 and conv_shortcut == True:\n",
    "                filters *= 2\n",
    "                x = basic_block(x, filters, conv_shortcut)\n",
    "            else:\n",
    "                x = basic_block(x, filters)\n",
    "        return x, filters\n",
    "    inputs = keras.Input(shape=(resizing, resizing, 3))\n",
    "    filters = 16\n",
    "    x = keras.layers.Conv2D(filters, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = keras.layers.BatchNormalization(epsilon=1.001e-5)(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x, filters = basic_stack(x, filters)\n",
    "    x, filters = basic_stack(x, filters, True)\n",
    "    x, filters = basic_stack(x, filters, True)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(rate)(x)\n",
    "    outputs = keras.layers.Dense(classes, activation='softmax')(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = load_cifar100(\n",
    "    batch_size=batch_size,\n",
    "    validation_batch_size=validation_batch_size,\n",
    "    resizing=resizing,\n",
    "    drop_remainder=drop_remainder,\n",
    "    num_parallel_calls=num_parallel_calls,\n",
    "    rand_augment=rand_augment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56163eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cifar_resnet(resizing=resizing, n=n, rate=rate, classes=classes)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a78fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr, milestones, gamma: float = 0.2):\n",
    "    if epoch in milestones:\n",
    "        lr *= gamma\n",
    "    return lr\n",
    "\n",
    "class TimeCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time_epoch_begin = time.perf_counter()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history.append(time.perf_counter() - self.time_epoch_begin)\n",
    "\n",
    "lr_scheduler_callback = keras.callbacks.LearningRateScheduler(\n",
    "    lambda x, y: lr_schedule(x, y, milestones=milestones, gamma=gamma)\n",
    ")\n",
    "time_callback = TimeCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    ),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    "    jit_compile=JIT_COMPILE_FLAG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449564bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training time warm-up\n",
    "'''\n",
    "class StopCallBack(keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.model.stop_training = True\n",
    "stop_call = StopCallBack()\n",
    "model.fit(dataloader['train'], verbose=2, callbacks=[stop_call])\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd619f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = model.fit(\n",
    "    dataloader['train'],\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    callbacks=[lr_scheduler_callback, time_callback],\n",
    "    validation_data=dataloader['test']\n",
    ")\n",
    "logs.history['t'] = time_callback.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5668c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = -1\n",
    "print('----')\n",
    "print('PRINT RESULTS')\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'MIXED_PRECISION: {MIXED_PRECISION_FLAG}')\n",
    "print(f'JIT_COMPILE: {JIT_COMPILE_FLAG}')\n",
    "print(f'time: {logs.history[\"t\"][index]}')\n",
    "print(f'learning_rate: {logs.history[\"lr\"][index]}')\n",
    "print(f'loss: {logs.history[\"loss\"][index]}')\n",
    "print(f'acc: {logs.history[\"accuracy\"][index]}')\n",
    "print(f'val_loss: {logs.history[\"val_loss\"][index]}')\n",
    "print(f'val_acc: {logs.history[\"val_accuracy\"][index]}')\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs = 512\n",
    "#Epoch 160/160\n",
    "#97/97 - 20s - loss: 1.2973 - accuracy: 0.6332 - val_loss: 1.2738 - val_accuracy: 0.6409 - lr: 1.0000e-03 - 20s/epoch - 205ms/step\n",
    "\n",
    "# bs = 256\n",
    "#Epoch 160/160\n",
    "#390/390 - 20s - loss: 1.1997 - accuracy: 0.6605 - val_loss: 1.2067 - val_accuracy: 0.6594 - lr: 1.0000e-03 - 20s/epoch - 51ms/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11354a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
