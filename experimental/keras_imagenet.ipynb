{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e13ab4-d4b2-434d-b765-69bafa975913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c71fb-acee-439e-835e-54ec59028f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for device\n",
    "device_index = 0\n",
    "MIXED_PRECISION_FLAG = True\n",
    "## Tensorflow 2.6 does not support jit_compile in model.compile() yet.\n",
    "## So, just set it to False.\n",
    "JIT_COMPILE_FLAG = False\n",
    "\n",
    "# for dataset\n",
    "dataset = 'imagenet'\n",
    "dir_path = f'/ssd/{dataset}'\n",
    "## r_b = [(160, 510), (224, 390), (288, 180)]\n",
    "resolution = 224\n",
    "batch_size = 1500\n",
    "\n",
    "# for model\n",
    "depth = 18\n",
    "# dropout rate from 0.1 to 0.3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# for training\n",
    "learning_rate = 1e-1\n",
    "momentum = 0.9\n",
    "epochs = 120\n",
    "## Tensorflow 2.6 does not support weight_decay in keras.optimizers.SGD() yet.\n",
    "## So, it might be set in the model.\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# for learning rate scheduler\n",
    "milestones = [30, 60, 90]\n",
    "gamma = 0.1\n",
    "\n",
    "######## for testing: BS and LR are propotional.\n",
    "learning_rate *= batch_size / 390\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bb295-8198-4b63-b497-b7a8fecda9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f'Numbers of Physical Devices: {len(physical_devices)}')\n",
    "tf.config.set_visible_devices(physical_devices[device_index], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[device_index], True)\n",
    "print(f'Using device: {physical_devices[device_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f4132-04be-4dd3-b99c-6e6818462370",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MIXED_PRECISION_FLAG:\n",
    "    policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "    keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f'Policy: {policy.name}')\n",
    "    print(f'Compute dtype: {policy.compute_dtype}')\n",
    "    print(f'Variable dtype: {policy.variable_dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet(resolution: int, batch_size: int):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.299, 0.224, 0.225]\n",
    "    var = [0.089401, 0.050176, 0.050625]#tf.math.square(std)\n",
    "    \n",
    "    resolution_list = [160, 224, 288]\n",
    "    if resolution not in resolution_list:\n",
    "        raise ValueError(f'Invalid resolution \"{resolution}\", it should be in {resolution_list}.')\n",
    "    \n",
    "    def train_map(image):\n",
    "        transform = keras.Sequential([\n",
    "            # tf.keras.utils.image_dataset_from_directory() can not allow random preprocessing layers\n",
    "            # move random preprocessing layers to build_model\n",
    "            #keras.layers.RandomFlip('horizontal'),\n",
    "            #keras.layers.RandomTranslation(0.125, 0.125, fill_mode='constant'),\n",
    "            keras.layers.Rescaling(1/255),\n",
    "            keras.layers.Normalization(mean=mean, variance=var)\n",
    "        ])\n",
    "        return transform(image)\n",
    "    train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=f'{dir_path}/train',\n",
    "        label_mode='int', # for keras.losses.SparseCategoricalCrossentropy()\n",
    "        batch_size=batch_size,\n",
    "        image_size=(resolution, resolution)\n",
    "    )\n",
    "    # tf.data.cache() is a bomb, causing excessive memory usage when training imagenet\n",
    "    train_data = train_data.map(lambda x, y: (train_map(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_data = train_data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    def val_map(image):\n",
    "        transform = keras.Sequential([\n",
    "            keras.layers.Rescaling(1/255),\n",
    "            keras.layers.Normalization(mean=mean, variance=var)\n",
    "        ])\n",
    "        return transform(image)\n",
    "    val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=f'{dir_path}/val',\n",
    "        label_mode='int', # for keras.losses.SparseCategoricalCrossentropy()\n",
    "        batch_size=batch_size,\n",
    "        image_size=(resolution, resolution)\n",
    "    )\n",
    "    # tf.data.cache() is a bomb, causing excessive memory usage when training imagenet\n",
    "    val_data = val_data.map(lambda x, y: (val_map(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_data = val_data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataloader = {\n",
    "        'train': train_data,\n",
    "        'val': val_data\n",
    "    }\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(\n",
    "    dataset: str,\n",
    "    depth: int,\n",
    "    resolution: int,\n",
    "    dropout_rate: float = 0.2,\n",
    ") -> keras.Model:\n",
    "    \n",
    "    dataset_list = ['cifar', 'imagenet']\n",
    "    depth_list = [18, 34]\n",
    "    \n",
    "    if dataset not in dataset_list:\n",
    "        raise ValueError(f'Invalid dataset \"{dataset}\", it should be in {dataset_list}.')\n",
    "    if depth not in depth_list:\n",
    "        raise ValueError(f'Invalid depth \"{depth}\", it should be in {depth_list}.')\n",
    "    \n",
    "    if dataset == 'cifar':\n",
    "        classes = 100\n",
    "    elif dataset == 'imagenet':\n",
    "        classes = 1000\n",
    "    \n",
    "    if depth == 18:\n",
    "        stack_list = [2, 2, 2, 2]\n",
    "    elif depth == 34:\n",
    "        stack_list = [3, 4, 6, 3]\n",
    "    \n",
    "    def basic_block(x: keras.Input, filters: int, conv_shortcut: bool = False):\n",
    "        if conv_shortcut:\n",
    "            shortcut = keras.layers.Conv2D(\n",
    "                filters, 1, strides=2, use_bias=False,\n",
    "                kernel_initializer='he_normal',\n",
    "                #kernel_regularizer=keras.regularizers.L2(1e-4)\n",
    "            )(x)\n",
    "            shortcut = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(shortcut)\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                kernel_initializer='he_normal',\n",
    "                #kernel_regularizer=keras.regularizers.L2(1e-4)\n",
    "            )(x)\n",
    "        else:\n",
    "            shortcut = x\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters, 3, padding='same', use_bias=False,\n",
    "                kernel_initializer='he_normal',\n",
    "                #kernel_regularizer=keras.regularizers.L2(1e-4)\n",
    "            )(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters, 3, padding='same', use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            #kernel_regularizer=keras.regularizers.L2(1e-4)\n",
    "        )(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Add()([shortcut, x])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    def basic_stack(x: keras.Input, filters: int, stack: int, conv_shortcut: bool = False):\n",
    "        for i in range(stack):\n",
    "            if i == 0 and conv_shortcut == True:\n",
    "                filters *= 2\n",
    "                x = basic_block(x, filters, conv_shortcut)\n",
    "            else:\n",
    "                x = basic_block(x, filters)\n",
    "        return x, filters\n",
    "    \n",
    "    inputs = keras.Input(shape=(resolution, resolution, 3))\n",
    "    filters = 64\n",
    "    ## random preprocessing layers\n",
    "    x = keras.layers.RandomFlip('horizontal')(inputs)\n",
    "    x = keras.layers.RandomTranslation(0.125, 0.125, fill_mode='constant')(x)\n",
    "    ## stem\n",
    "    if dataset == 'cifar':\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters, 3, padding='same', use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            #kernel_regularizer=keras.regularizers.L2(1e-4)\n",
    "        )(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "    elif dataset == 'imagenet':\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters, 7, strides=2, padding='same', use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            #kernel_regularizer=keras.regularizers.L2(1e-4)\n",
    "        )(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        x = keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    ## trunk\n",
    "    for i, stack in enumerate(stack_list):\n",
    "        if i == 0:\n",
    "            x, filters = basic_stack(x, filters, stack)\n",
    "        else:\n",
    "            x, filters = basic_stack(x, filters, stack, True)\n",
    "    ## classifier\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    outputs = keras.layers.Dense(classes, activation='softmax')(x)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53337f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = load_imagenet(resolution=resolution, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adffc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_resnet(dataset=dataset, depth=depth, resolution=resolution, dropout_rate=dropout_rate)\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr, milestones, gamma: float = 0.1):\n",
    "    if epoch in milestones:\n",
    "        lr *= gamma\n",
    "    return lr\n",
    "\n",
    "class TimeCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time_epoch_begin = time.perf_counter()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history.append(time.perf_counter() - self.time_epoch_begin)\n",
    "\n",
    "lr_scheduler_callback = keras.callbacks.LearningRateScheduler(\n",
    "    lambda x, y: lr_schedule(x, y, milestones=milestones, gamma=gamma)\n",
    ")\n",
    "\n",
    "time_callback = TimeCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9773bd5-e1d0-45aa-b110-8ee02501fb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    "    #jit_compile=JIT_COMPILE_FLAG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2cde0-014f-4d0d-8295-5f24d940d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = model.fit(\n",
    "    dataloader['train'],\n",
    "    epochs=epochs,\n",
    "    verbose='auto',\n",
    "    callbacks=[time_callback, lr_scheduler_callback],\n",
    "    validation_data=dataloader['val']\n",
    ")\n",
    "logs.history['t'] = time_callback.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5830196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3367826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
