{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e13ab4-d4b2-434d-b765-69bafa975913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tf_data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.MetavarTypeHelpFormatter):\n",
    "    pass\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Progressive Training by Using Tensorflow',\n",
    "    epilog=(\n",
    "        'The parser only supports high-level control options. '\n",
    "        'If the user wants to adjust low-level control options, modify the code. '\n",
    "        'Required sttings [--dataset, --path] or [-d, -p], optional settings [--amp, --xla].'\n",
    "    ),\n",
    "    formatter_class=CustomFormatter,\n",
    ")\n",
    "\n",
    "# high-level control options\n",
    "parser.add_argument(\n",
    "    '--multi-gpu',\n",
    "    action='store_true',\n",
    "    help='training using multiple GPUs, not yet completed',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--mixed-precision', '--amp',\n",
    "    action='store_true',\n",
    "    help='train with mixed precision (amp)',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--jit-compile', '--xla',\n",
    "    action='store_true',\n",
    "    help='train with jit compile (xla)',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--dataset', '--data', '-d',\n",
    "    type=str,\n",
    "    help='dataset to train, currently supports [\"cifar10\", \"cifar100\", \"imagenet\"]',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--dir-path', '--path', '-p',\n",
    "    type=str,\n",
    "    help='path to the dataset directory',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--comments', '-c',\n",
    "    type=str,\n",
    "    help='add additional comments on filename',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--no-cycle',\n",
    "    dest='cycle',\n",
    "    action='store_false',\n",
    "    help='do not use all image resolutions with different learning rates',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--no-temp',\n",
    "    dest='temp',\n",
    "    action='store_false',\n",
    "    help='do not save the temporary state during training, including \"_model\" and \".npy\"',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--no-save',\n",
    "    dest='save',\n",
    "    action='store_false',\n",
    "    help='do not save the training results, including \"_model\" and \".npy\"',\n",
    ")\n",
    "\n",
    "# check the file type is '.py' or '.ipynb'\n",
    "## parse args of '.ipynb' from here\n",
    "## ex. ['--dataset=imagenet', '--path=./dataset', '--amp', '--xla']\n",
    "ipynb_args = ['-d=cifar100', '-p=~/ssd', '--amp', '--xla']\n",
    "args = (\n",
    "    parser.parse_args(ipynb_args)\n",
    "    if len(sys.argv) > 2 and sys.argv[1] == '-f' and '.json' in sys.argv[2]\n",
    "    else parser.parse_args()\n",
    ")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c71fb-acee-439e-835e-54ec59028f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-level control options\n",
    "## device\n",
    "### data perallel not complete yet, set \"False\"\n",
    "### there exists a lot of problems to solve, which are incompatible:\n",
    "### - tf.distribute.MirroredStrategy()\n",
    "### - keras.backend.clear_session()\n",
    "### - jit_compile\n",
    "MULTI_GPU = args.multi_gpu\n",
    "MIXED_PRECISION = args.mixed_precision\n",
    "### TF 2.6 does not support jit_compile in keras.Model.compile() yet\n",
    "### another way is to use environment variable 'TF_XLA_FLAGS'\n",
    "### set os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'\n",
    "JIT_COMPILE = args.jit_compile\n",
    "## dataset and model\n",
    "dataset = args.dataset\n",
    "dir_path = args.dir_path\n",
    "## training\n",
    "CYCLE = args.cycle\n",
    "## output file\n",
    "TEMP = args.temp\n",
    "SAVE = args.save\n",
    "\n",
    "# low-level control options\n",
    "## device\n",
    "device_index = 0\n",
    "## dataset and model\n",
    "depth = 18\n",
    "if dataset == 'cifar10' or dataset == 'cifar100': \n",
    "    batch_size_ls = [1000, 500]\n",
    "    resolution_ls = [24, 32]\n",
    "    dropout_rate_ls = [0.1, 0.2]\n",
    "elif dataset == 'imagenet':\n",
    "    batch_size_ls = [510, 360, 170]\n",
    "    resolution_ls = [160, 224, 288]\n",
    "    dropout_rate_ls = [0.1, 0.2, 0.3]\n",
    "else:\n",
    "    raise ValueError(f'Invalid dataset \"{dataset}\".')\n",
    "## training\n",
    "learning_rate = 1e-1\n",
    "momentum = 0.9\n",
    "### TF 2.6 does not support weight_decay in keras.optimizers.SGD() yet\n",
    "### another way is to modify the model, which is set in tf_data_model.py\n",
    "weight_decay = 1e-4 # [None, 1e-4]\n",
    "epochs = 90\n",
    "step = 3\n",
    "gamma = 0.1\n",
    "## output file\n",
    "\n",
    "# adaptive options\n",
    "## device\n",
    "## dataset and model\n",
    "batch_size_iter = itertools.cycle(batch_size_ls)\n",
    "resolution_iter = itertools.cycle(resolution_ls)\n",
    "dropout_rate_iter = itertools.cycle(dropout_rate_ls)\n",
    "## training\n",
    "milestones = list(int(epochs * i / step) for i in range(1, step))\n",
    "modify_freq = int((milestones[0] if CYCLE else epochs) / len(resolution_ls))\n",
    "## output file\n",
    "outfile = (\n",
    "    f'{dataset}_resnet{depth}_{epochs}'\n",
    "    f'{\"_amp\" if MIXED_PRECISION else \"\"}'\n",
    "    f'{\"_xla\" if JIT_COMPILE else \"\"}'\n",
    "    f'{\"\" if CYCLE else \"_nocycle\"}'\n",
    "    f'{\"_\" + args.comments if args.comments else \"\"}'\n",
    ")\n",
    "tempfile = f'temp_{outfile}'\n",
    "\n",
    "# experimental: BS and LR are propotional\n",
    "#learning_rate *= batch_size / (32 * 8) # PyTorch uses batch size 32 with 8 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_precision and jit_compile\n",
    "## experimental: TF 2.6 jit_compile\n",
    "### it must be called berfore any tensorflow function\n",
    "### for unknown reasons, '--tf_xla_cpu_global_jit' only supports the first GPU.\n",
    "### otherwise an error will result.\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_cpu_global_jit' if JIT_COMPILE else ''\n",
    "tf.config.optimizer.set_jit('autoclustering' if JIT_COMPILE else False)\n",
    "print(f'Optimizer set_jit: \"{tf.config.optimizer.get_jit()}\"')\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "    keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f'Policy: {policy.name}')\n",
    "    print(f'Compute dtype: {policy.compute_dtype}')\n",
    "    print(f'Variable dtype: {policy.variable_dtype}')\n",
    "\n",
    "print('----')\n",
    "print(f'MIXED_PRECISION: {MIXED_PRECISION}')\n",
    "print(f'JIT_COMPILE: {JIT_COMPILE}')\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bb295-8198-4b63-b497-b7a8fecda9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## data perallel not complete yet\n",
    "# GPU initialization\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f'The Number of Available Physical Devices: {len(physical_devices)}')\n",
    "tf.config.set_visible_devices(\n",
    "    physical_devices[:] if MULTI_GPU else physical_devices[device_index],\n",
    "    'GPU'\n",
    ")\n",
    "for device in tf.config.get_visible_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "print(f'Using Devices: {tf.config.get_visible_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr, milestones, gamma: float = 0.1):\n",
    "    if epoch in milestones:\n",
    "        lr *= gamma\n",
    "    return lr\n",
    "\n",
    "class TimeCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time_epoch_begin = time.perf_counter()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history.append(time.perf_counter() - self.time_epoch_begin)\n",
    "\n",
    "def get_modified_parameters(\n",
    "    batch_size_iter: itertools.cycle,\n",
    "    resolution_iter: itertools.cycle,\n",
    "    dropout_rate_iter: itertools.cycle,\n",
    "    learning_rate: float,\n",
    "    logs: dict,\n",
    "    modify_freq: int\n",
    "):\n",
    "    batch_size = next(batch_size_iter)\n",
    "    resolution = next(resolution_iter)\n",
    "    dropout_rate = next(dropout_rate_iter)\n",
    "    logs['batch_size'] += list(itertools.repeat(batch_size, modify_freq))\n",
    "    logs['resolution'] += list(itertools.repeat(resolution, modify_freq))\n",
    "    logs['dropout_rate'] += list(itertools.repeat(dropout_rate, modify_freq))\n",
    "    if 'lr' in logs.keys():\n",
    "        learning_rate = logs['lr'][-1]\n",
    "    print(f'batch_size: {batch_size}')\n",
    "    print(f'resolution: {resolution}, dropout_rate: {dropout_rate}')\n",
    "    print(f'learning_rate (previous step): {learning_rate: g}')\n",
    "    return batch_size, resolution, dropout_rate, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_callback = keras.callbacks.LearningRateScheduler(\n",
    "    lambda x, y: lr_schedule(x, y, milestones=milestones, gamma=gamma)\n",
    ")\n",
    "time_callback = TimeCallback()\n",
    "logs = {\n",
    "    'batch_size': [],\n",
    "    'resolution': [],\n",
    "    'dropout_rate': [],\n",
    "    't': []\n",
    "}\n",
    "epoch_index = len(logs['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bec94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch_index < epochs:\n",
    "    # modify data and model\n",
    "    batch_size, resolution, dropout_rate, learning_rate = get_modified_parameters(\n",
    "        batch_size_iter, resolution_iter, dropout_rate_iter, learning_rate, logs, modify_freq\n",
    "    )\n",
    "    dataloader = tf_data_model.load_data(\n",
    "        resolution=resolution,\n",
    "        batch_size=batch_size,\n",
    "        dataset=dataset,\n",
    "        dir_path=dir_path\n",
    "    )\n",
    "    model = tf_data_model.modify_resnet(\n",
    "        dataset=dataset,\n",
    "        depth=depth,\n",
    "        dropout_rate=dropout_rate,\n",
    "        resolution=resolution,\n",
    "        old_model=model if epoch_index else None\n",
    "    )\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.experimental.SGD(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=momentum,\n",
    "            weight_decay=None if tf_data_model.OLD_VERSION else weight_decay\n",
    "            ## 'decay_steps' in 'keras.optimizers.schedules.LearningRateSchedule()'\n",
    "            ## means batches instead of epochs, which is a fine grained value\n",
    "            ## so try to use 'keras.callbacks.LearningRateScheduler()'\n",
    "            ## to set the learning rate decay value each epoch.\n",
    "        ),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # training step, record temporary logs\n",
    "    temp_logs = model.fit(\n",
    "        dataloader['train'],\n",
    "        epochs=min(epoch_index + modify_freq, epochs),\n",
    "        verbose='auto',\n",
    "        callbacks=[time_callback, lr_scheduler_callback],\n",
    "        validation_data=dataloader['val'],\n",
    "        initial_epoch=epoch_index\n",
    "    )\n",
    "    temp_logs.history['t'] = time_callback.history\n",
    "\n",
    "    # concatenate temporary logs to the logs\n",
    "    for key, value in temp_logs.history.items():\n",
    "        if key in logs:\n",
    "            logs[key] += value\n",
    "        else:\n",
    "            logs[key] = value\n",
    "    \n",
    "    # update epoch index of the training\n",
    "    epoch_index = len(logs['t'])\n",
    "\n",
    "    # save the temporary state (aka checkpoint)\n",
    "    if TEMP and epoch_index < epochs:\n",
    "        keras.models.save_model(model, f'{tempfile}_model')\n",
    "        np.save(f'{tempfile}.npy', logs)\n",
    "        print(f'Save The Temporary State at Epoch {epoch_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    keras.models.save_model(model, f'{outfile}_model')\n",
    "    np.save(f'{outfile}.npy', logs)\n",
    "    print(f'Save Model: {outfile}_model')\n",
    "    print(f'Save Logs: {outfile}.npy')\n",
    "\n",
    "if TEMP:\n",
    "    shutil.rmtree(f'{tempfile}_model', ignore_errors=True)\n",
    "    os.remove(f'{tempfile}.npy') if os.path.isfile(f'{tempfile}.npy') else None\n",
    "    print('Clean The Temporary State')\n",
    "\n",
    "print('Training Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07046c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
