{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e13ab4-d4b2-434d-b765-69bafa975913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tf_data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c71fb-acee-439e-835e-54ec59028f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for device\n",
    "device_index = 0\n",
    "MIXED_PRECISION_FLAG = True\n",
    "## TF 2.6 does not support jit_compile in keras.Model.compile() yet.\n",
    "## So, just set it to False.\n",
    "## Another way is to use environment variable 'TF_XLA_FLAGS'.\n",
    "## Set os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'.\n",
    "JIT_COMPILE_FLAG = True\n",
    "\n",
    "# for dataset\n",
    "dataset = 'cifar100'\n",
    "dir_path = '/ssd'\n",
    "## r_b = [(160, 510), (224, 390), (288, 180)] without jit_compile\n",
    "## r_b = [(160, 510), (224, 360), (288, 170)] with jit_compile\n",
    "batch_size = 500\n",
    "resolution_iter = itertools.cycle([24, 32]) # [160, 224, 288]\n",
    "\n",
    "# for model\n",
    "depth = 18\n",
    "dropout_rate_iter = itertools.cycle([0.1, 0.2]) # [0.1, 0.2, 0.3]\n",
    "\n",
    "# for training\n",
    "learning_rate = 1e-1\n",
    "momentum = 0.9\n",
    "epochs = 90 # 120\n",
    "modify_freq = 45 # 10\n",
    "## TF 2.6 does not support weight_decay in keras.optimizers.SGD() yet.\n",
    "## So, it might be set in the model.\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# for learning rate scheduler\n",
    "milestones = [30, 60] # [30, 60, 90]\n",
    "gamma = 0.1\n",
    "\n",
    "# create for using later\n",
    "def get_modify_parameters(resolution_iter: itertools.cycle, dropout_rate_iter: itertools.cycle):\n",
    "    resolution = next(resolution_iter)\n",
    "    dropout_rate = next(dropout_rate_iter)\n",
    "    print(f'resolution: {resolution}, dropout_rate: {dropout_rate}')\n",
    "    return resolution, dropout_rate\n",
    "logs = {}\n",
    "epoch_index = len(logs['t']) if 't' in logs.keys() else 0\n",
    "\n",
    "######## for testing: BS and LR are propotional\n",
    "#learning_rate *= batch_size / (32 * 8) # PyTorch uses batch size 32 with 8 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## for testing: TF 2.6 jit_compile, it must call berfore any tensorflow function.\n",
    "## For unknown reasons, '--tf_xla_cpu_global_jit' only supports the first GPU.\n",
    "## Otherwise an error will result.\n",
    "if JIT_COMPILE_FLAG:\n",
    "    if device_index == 0:\n",
    "        # can not use the condition 'len(tf.config.list_physical_devices('GPU')) == 1'\n",
    "        # since it calls tf function...\n",
    "        os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'\n",
    "    else:\n",
    "        os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'\n",
    "\n",
    "if MIXED_PRECISION_FLAG:\n",
    "    policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "    keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f'Policy: {policy.name}')\n",
    "    print(f'Compute dtype: {policy.compute_dtype}')\n",
    "    print(f'Variable dtype: {policy.variable_dtype}')\n",
    "\n",
    "print('----')\n",
    "print(f'MIXED_PRECISION: {MIXED_PRECISION_FLAG}')\n",
    "print(f'JIT_COMPILE: {JIT_COMPILE_FLAG}')\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bb295-8198-4b63-b497-b7a8fecda9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f'Numbers of Physical Devices: {len(physical_devices)}')\n",
    "tf.config.set_visible_devices(physical_devices[device_index], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[device_index], True)\n",
    "print(f'Using Device: {physical_devices[device_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr, milestones, gamma: float = 0.1):\n",
    "    if epoch in milestones:\n",
    "        lr *= gamma\n",
    "    return lr\n",
    "\n",
    "class TimeCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time_epoch_begin = time.perf_counter()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history.append(time.perf_counter() - self.time_epoch_begin)\n",
    "\n",
    "lr_scheduler_callback = keras.callbacks.LearningRateScheduler(\n",
    "    lambda x, y: lr_schedule(x, y, milestones=milestones, gamma=gamma)\n",
    ")\n",
    "\n",
    "time_callback = TimeCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bec94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch_index < epochs:\n",
    "    resolution, dropout_rate = get_modify_parameters(resolution_iter, dropout_rate_iter)\n",
    "    dataloader = tf_data_model.load_cifar(\n",
    "        resolution=resolution, batch_size=batch_size, dataset=dataset\n",
    "    )\n",
    "    model = tf_data_model.modify_resnet(\n",
    "        old_model=model if epoch_index else None,\n",
    "        dataset=dataset,\n",
    "        depth=depth,\n",
    "        dropout_rate=dropout_rate,\n",
    "        resolution=resolution\n",
    "    )\n",
    "    \n",
    "    if 'lr' in logs.keys():\n",
    "        learning_rate = logs['lr'][-1]\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=momentum,\n",
    "            #weight_decay=weight_decay\n",
    "        ),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy'],\n",
    "        #jit_compile=JIT_COMPILE_FLAG\n",
    "    )\n",
    "    print(f'learning_rate: {learning_rate}')\n",
    "    \n",
    "    temp_logs = model.fit(\n",
    "        dataloader['train'],\n",
    "        epochs=epoch_index + modify_freq,\n",
    "        verbose='auto',\n",
    "        callbacks=[time_callback, lr_scheduler_callback],\n",
    "        validation_data=dataloader['val'],\n",
    "        initial_epoch=epoch_index\n",
    "    )\n",
    "    temp_logs.history['t'] = time_callback.history\n",
    "\n",
    "    for key, value in temp_logs.history.items():\n",
    "        if key in logs:\n",
    "            logs[key] += value\n",
    "        else:\n",
    "            logs[key] = value\n",
    "    \n",
    "    epoch_index = len(logs['t']) if 't' in logs.keys() else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd1249",
   "metadata": {},
   "source": [
    "- cifar100\n",
    "    - for each lr, train all resolution\n",
    "        - 90 epoch, resolution = [24, 32], milestones = [30, 60]\n",
    "        - loss: 0.7437 - accuracy: 0.9930 - val_loss: 2.0937 - val_accuracy: 0.6790\n",
    "    - seperate lr and resolution\n",
    "        - 90 epoch, resolution = [24, 32], res_milestones = [45], lr_milestones = [30, 60]\n",
    "        - loss: 0.9103 - accuracy: 0.9633 - val_loss: 2.3247 - val_accuracy: 0.6400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a7a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
