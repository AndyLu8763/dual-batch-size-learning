{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aca3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tf_data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.MetavarTypeHelpFormatter):\n",
    "    pass\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Record Training Time by Using Tensorflow',\n",
    "    epilog=(\n",
    "        'Required settings [--resolution, --dataset, --path] or [-r, -d, -p], '\n",
    "        'optional for loop arguments [--start, --stop, --step], '\n",
    "        'optional settings [--amp, --xla, --depth, --take, --comments]'\n",
    "    ),\n",
    "    formatter_class=CustomFormatter,\n",
    ")\n",
    "\n",
    "# parser arguments\n",
    "## GPU\n",
    "parser.add_argument(\n",
    "    '--device-index',\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help='the index of the GPU used to run the program, \"0\" or \"-1\" is a good choice',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--mixed-precision', '--amp',\n",
    "    dest='amp',\n",
    "    action='store_true',\n",
    "    help='train with mixed precision (amp)',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--jit-compile', '--xla',\n",
    "    dest='xla',\n",
    "    action='store_true',\n",
    "    help='train with jit compile (xla)',\n",
    ")\n",
    "## dataset and model\n",
    "parser.add_argument(\n",
    "    '--resolution', '-r',\n",
    "    type=int,\n",
    "    help='image resolution, currently support [24, 32] for \"cifar\" && [160, 224, 288] for \"imagenet\"',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--dataset', '--data', '-d',\n",
    "    type=str,\n",
    "    help='dataset to train, currently supports [\"cifar10\", \"cifar100\", \"imagenet\"]',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--dir-path', '--path', '-p',\n",
    "    type=str,\n",
    "    help='path to the dataset directory',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--depth',\n",
    "    type=int,\n",
    "    default=18,\n",
    "    help='resnet depth, currently supports [18, 34]',\n",
    ")\n",
    "## for loop\n",
    "parser.add_argument(\n",
    "    '--start',\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help='\"start\" value for range() in the for loop',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--stop',\n",
    "    type=int,\n",
    "    default=101,\n",
    "    help='\"stop\" value for range() in the for loop',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--step',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='\"step\" value for range() in the for loop',\n",
    ")\n",
    "## others\n",
    "parser.add_argument(\n",
    "    '--take', '-t',\n",
    "    type=int,\n",
    "    help='creates a \"Dataset\" with at most \"count\" elements from this dataset, could be [int, None]',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--comments', '-c',\n",
    "    type=str,\n",
    "    help='add additional comments on filename',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--no-save',\n",
    "    dest='save',\n",
    "    action='store_false',\n",
    "    help='do not save the training results, including \"_model\" and \".npy\"',\n",
    ")\n",
    "\n",
    "# check the file type is '.py' or '.ipynb'\n",
    "## parse args of '.ipynb' from here\n",
    "## ex. ['--dataset=imagenet', '--path=./dataset', '--amp', '--xla']\n",
    "ipynb_args = [\n",
    "    '-r=32', '-d=cifar100', '-p=/ssd',\n",
    "    '--start=1000', '--stop=2001', '--step=10000',\n",
    "    '-t=10', '--amp', '--xla', '--no-save',\n",
    "]\n",
    "args = (\n",
    "    parser.parse_args(ipynb_args)\n",
    "    if len(sys.argv) > 2 and sys.argv[1] == '-f' and '.json' in sys.argv[2]\n",
    "    else parser.parse_args()\n",
    ")\n",
    "outfile = (\n",
    "    f'time_{args.dataset}_resnet{args.depth}_r{args.resolution}'\n",
    "    f'_{args.start}_{args.stop}_{args.step}'\n",
    "    f'{\"_amp\" if args.amp else \"\"}'\n",
    "    f'{\"_xla\" if args.xla else \"\"}'\n",
    ")\n",
    "print('----')\n",
    "print(outfile)\n",
    "print(args)\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2899b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_precision and jit_compile\n",
    "### for unknown reasons, '--tf_xla_cpu_global_jit' only supports the first GPU\n",
    "if args.xla:\n",
    "    os.environ['TF_XLA_FLAGS'] = '--tf_xla_cpu_global_jit'\n",
    "    tf.config.optimizer.set_jit('autoclustering')\n",
    "    print(f'Optimizer set_jit: \"{tf.config.optimizer.get_jit()}\"')\n",
    "\n",
    "if args.amp:\n",
    "    policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "    keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f'Policy: {policy.name}')\n",
    "    print(f'Compute dtype: {policy.compute_dtype}')\n",
    "    print(f'Variable dtype: {policy.variable_dtype}')\n",
    "\n",
    "print('----')\n",
    "print(f'MIXED_PRECISION: {args.amp}')\n",
    "print(f'JIT_COMPILE: {args.xla}')\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU initialization, data perallel not complete yet\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[args.device_index], 'GPU')\n",
    "for device in tf.config.get_visible_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "print('----')\n",
    "print(f'The Number of Available Physical Devices: {len(physical_devices)}')\n",
    "print(f'Using Devices: {tf.config.get_visible_devices(\"GPU\")}')\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time_epoch_begin = time.perf_counter()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history.append(time.perf_counter() - self.time_epoch_begin)\n",
    "\n",
    "time_callback = TimeCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {\n",
    "    'batch_size': [],\n",
    "    'total_train_time': [],\n",
    "    'avg_train_time': [],\n",
    "    'take': args.take,\n",
    "    'data': args.dataset,\n",
    "    'depth': args.depth,\n",
    "    'resolution': args.resolution,\n",
    "    'amp': args.amp,\n",
    "    'xla': args.xla,\n",
    "    'sss': (args.start, args.stop, args.step),\n",
    "    'comments': args.comments,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fbbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in range(args.start, args.stop, args.step):\n",
    "    print(f'Batch Size = {batch_size}')\n",
    "    # data && model\n",
    "    dataloader = tf_data_model.load_data(\n",
    "        resolution=args.resolution,\n",
    "        batch_size=batch_size,\n",
    "        dataset=args.dataset,\n",
    "        dir_path=args.dir_path,\n",
    "    )\n",
    "    model = tf_data_model.modify_resnet(\n",
    "        dataset=args.dataset,\n",
    "        depth=args.depth,\n",
    "        dropout_rate=0,\n",
    "        resolution=args.resolution,\n",
    "        old_model=None,\n",
    "    )\n",
    "    # compile\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.experimental.SGD(\n",
    "            learning_rate=1e-1,\n",
    "            momentum=0.9,\n",
    "            weight_decay=None if tf_data_model.OLD_VERSION else 1e-4,\n",
    "        ),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    # warmup\n",
    "    print('Warmup:', end=' ')\n",
    "    model.fit(\n",
    "        dataloader['train'].take(1),\n",
    "        verbose=2,\n",
    "    )\n",
    "    # train\n",
    "    print('Train:', end=' ')\n",
    "    temp_logs = model.fit(\n",
    "        dataloader['train'].take(args.take) if args.take else dataloader['train'],\n",
    "        verbose=2,\n",
    "        callbacks=[time_callback],\n",
    "    )\n",
    "    # record\n",
    "    logs['batch_size'].append(batch_size)\n",
    "    logs['total_train_time'].append(time_callback.history[0])\n",
    "    if args.take:  # for data with limited batches\n",
    "        logs['avg_train_time'].append(time_callback.history[0] / args.take)\n",
    "    elif args.dataset == 'imagenet':  # for total 'imagenet' dataset\n",
    "        logs['avg_train_time'].append(time_callback.history[0] / math.ceil(1281167 / batch_size))\n",
    "    else:  # for total 'cifar' dataset\n",
    "        logs['avg_train_time'].append(time_callback.history[0] / math.ceil(50000 / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "if args.save:\n",
    "    np.save(f'{outfile}.npy', logs)\n",
    "    print(f'Save Logs: {outfile}.npy')\n",
    "\n",
    "# load file\n",
    "if False:\n",
    "    f = np.load(f'{outfile}.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae03a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
