{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e13ab4-d4b2-434d-b765-69bafa975913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import imgaug as ia\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139fbd1e",
   "metadata": {},
   "source": [
    "## RandAugment Is Suck in TensorFlow\n",
    "### Create TensorFlow `Dataset` objects\n",
    "- https://keras.io/examples/vision/randaugment/#create-tensorflow-dataset-objects\n",
    "\n",
    "- Because `RandAugment` can only process NumPy arrays, it cannot be applied directly as part of the `Dataset` object (which expects TensorFlow tensors). To make `RandAugment` part of the dataset, we need to wrap it in a [tf.py_function](https://www.tensorflow.org/api_docs/python/tf/py_function).\n",
    "\n",
    "- A `tf.py_function` is a TensorFlow operation (which, like any other TensorFlow operation, takes TF tensors as arguments and returns TensorFlow tensors) that is capable of running arbitrary Python code. Naturally, this Python code can only be executed on CPU (whereas the rest of the TensorFlow graph can be accelerated on GPU), which in some cases can cause significant slowdowns -- however, in this case, the `Dataset` pipeline will run asynchronously together with the model, and doing preprocessing on CPU will remain performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c71fb-acee-439e-835e-54ec59028f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for device\n",
    "device_index = -1\n",
    "MIXED_PRECISION_FLAG = True\n",
    "## TF 2.6 does not support jit_compile in keras.Model.compile() yet.\n",
    "## So, just set it to False.\n",
    "## Another way is to use environment variable 'TF_XLA_FLAGS'.\n",
    "## Set os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'.\n",
    "JIT_COMPILE_FLAG = True\n",
    "\n",
    "# for dataset\n",
    "dataset = 'imagenet'\n",
    "dir_path = '/ssd'\n",
    "## r_b = [(160, 510), (224, 390), (288, 180)] without jit_compile\n",
    "## r_b = [(160, 510), (224, 360), (288, 170)] with jit_compile\n",
    "resolution = 224\n",
    "batch_size = 512\n",
    "\n",
    "# for model\n",
    "depth = 18\n",
    "dropout_rate = 0.2 # [0.1, 0.2, 0.3]\n",
    "\n",
    "# for training\n",
    "learning_rate = 1e-1\n",
    "momentum = 0.9\n",
    "epochs = 120 # 120\n",
    "## TF 2.6 does not support weight_decay in keras.optimizers.SGD() yet.\n",
    "## So, it might be set in the model.\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# for learning rate scheduler\n",
    "milestones = [30, 60, 90] # [30, 60, 90]\n",
    "gamma = 0.1\n",
    "\n",
    "######## for testing: BS and LR are propotional\n",
    "learning_rate *= batch_size / 128\n",
    "######## for testing: kernel_regularizer for TF 2.6\n",
    "KR_FLAG = True\n",
    "KR_VALUE = keras.regularizers.L2(weight_decay) if KR_FLAG else None\n",
    "######## for testing: RandAugment\n",
    "JIT_COMPILE_FLAG = False # jit_compile \"must\" be \"False\"\n",
    "num_augment = 1\n",
    "magnitude_augment = 10 # [5, 10 ,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## for testing: TF 2.6 jit_compile, it must call berfore any tensorflow function.\n",
    "## For unknown reasons, '--tf_xla_cpu_global_jit' only supports the first GPU.\n",
    "## Otherwise an error will result.\n",
    "if JIT_COMPILE_FLAG:\n",
    "    if device_index == 0:\n",
    "        # can not use the condition 'len(tf.config.list_physical_devices('GPU')) == 1'\n",
    "        # since it calls tf function...\n",
    "        os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'\n",
    "    else:\n",
    "        os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'\n",
    "\n",
    "if MIXED_PRECISION_FLAG:\n",
    "    policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "    keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f'Policy: {policy.name}')\n",
    "    print(f'Compute dtype: {policy.compute_dtype}')\n",
    "    print(f'Variable dtype: {policy.variable_dtype}')\n",
    "\n",
    "print('----')\n",
    "print(f'MIXED_PRECISION: {MIXED_PRECISION_FLAG}')\n",
    "print(f'JIT_COMPILE: {JIT_COMPILE_FLAG}')\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bb295-8198-4b63-b497-b7a8fecda9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f'Numbers of Physical Devices: {len(physical_devices)}')\n",
    "tf.config.set_visible_devices(physical_devices[device_index], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[device_index], True)\n",
    "print(f'Using Device: {physical_devices[device_index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet(\n",
    "    resolution: int, batch_size: int,\n",
    "    num_augment: int = 2, magnitude_augment: int = 10\n",
    "):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.299, 0.224, 0.225]\n",
    "    var = [0.089401, 0.050176, 0.050625] # tf.math.square(std)\n",
    "    \n",
    "    resolution_list = [160, 224, 288]\n",
    "    \n",
    "    if resolution not in resolution_list:\n",
    "        raise ValueError(f'Invalid resolution \"{resolution}\", it should be in {resolution_list}.')\n",
    "    \n",
    "    '''\n",
    "    # tf.keras.utils.image_dataset_from_directory() can not allow simple augmentation pipeline\n",
    "    # simple augmentation pipeline == keras.layers.RandomXXX()\n",
    "    # move simple augmentation pipeline to build_model\n",
    "    simple_aug = keras.Sequential([\n",
    "        keras.layers.RandomFlip('horizontal'),\n",
    "        keras.layers.RandomRotation(factor=0.02),\n",
    "        keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2)\n",
    "    ])\n",
    "    '''\n",
    "    def preprocessing_map(image):\n",
    "        transform = keras.Sequential([\n",
    "            keras.layers.Rescaling(1/255),\n",
    "            keras.layers.Normalization(mean=mean, variance=var)\n",
    "        ])\n",
    "        return transform(image)\n",
    "    \n",
    "    rand_aug = ia.augmenters.RandAugment(n=num_augment, m=magnitude_augment)\n",
    "    def augment(images):\n",
    "        # Input to `augment()` is a TensorFlow tensor which is not supported by `imgaug`.\n",
    "        # This is why we first convert it to its `numpy` variant.\n",
    "        images = tf.cast(images, tf.uint8)\n",
    "        return rand_aug(images=images.numpy())\n",
    "    \n",
    "    dataloader = {\n",
    "        'train': (\n",
    "            tf.keras.utils.image_dataset_from_directory(\n",
    "                directory=f'{dir_path}/imagenet/train',\n",
    "                label_mode='int', # for keras.losses.SparseCategoricalCrossentropy()\n",
    "                batch_size=batch_size,\n",
    "                image_size=(resolution, resolution)\n",
    "            )\n",
    "            .map(\n",
    "                lambda x, y: (preprocessing_map(x), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "            # tf.data.cache() is a bomb, causing excessive memory usage when training imagenet\n",
    "            .map(\n",
    "                lambda x, y: (tf.py_function(augment, [x], [tf.float32])[0], y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        ),\n",
    "        'val': (\n",
    "            tf.keras.utils.image_dataset_from_directory(\n",
    "                directory=f'{dir_path}/imagenet/val',\n",
    "                label_mode='int', # for keras.losses.SparseCategoricalCrossentropy()\n",
    "                batch_size=batch_size,\n",
    "                image_size=(resolution, resolution)\n",
    "            )\n",
    "            .map(\n",
    "                lambda x, y: (preprocessing_map(x), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "            # tf.data.cache() is a bomb, causing excessive memory usage when training imagenet\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(\n",
    "    dataset: str,\n",
    "    depth: int,\n",
    "    dropout_rate: float,\n",
    "    resolution: int\n",
    ") -> keras.Model:\n",
    "    \n",
    "    dataset_list = ['cifar10', 'cifar100', 'imagenet']\n",
    "    depth_list = [18, 34]\n",
    "    \n",
    "    if dataset not in dataset_list:\n",
    "        raise ValueError(f'Invalid dataset \"{dataset}\", it should be in {dataset_list}.')\n",
    "    if depth not in depth_list:\n",
    "        raise ValueError(f'Invalid depth \"{depth}\", it should be in {depth_list}.')\n",
    "    \n",
    "    if dataset == 'cifar10':\n",
    "        classes = 10\n",
    "    elif dataset == 'cifar100':\n",
    "        classes = 100\n",
    "    elif dataset == 'imagenet':\n",
    "        classes = 1000\n",
    "    \n",
    "    if depth == 18:\n",
    "        stack_list = [2, 2, 2, 2]\n",
    "    elif depth == 34:\n",
    "        stack_list = [3, 4, 6, 3]\n",
    "    \n",
    "    def basic_block(x: keras.Input, filters: int, conv_shortcut: bool = False):\n",
    "        if conv_shortcut:\n",
    "            shortcut = keras.layers.Conv2D(\n",
    "                filters, 1, strides=2, use_bias=False,\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=KR_VALUE\n",
    "            )(x)\n",
    "            shortcut = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(shortcut)\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=KR_VALUE\n",
    "            )(x)\n",
    "        else:\n",
    "            shortcut = x\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters, 3, padding='same', use_bias=False,\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=KR_VALUE\n",
    "            )(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters, 3, padding='same', use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=KR_VALUE\n",
    "        )(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Add()([shortcut, x])\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    def basic_stack(x: keras.Input, filters: int, stack: int, conv_shortcut: bool = False):\n",
    "        for i in range(stack):\n",
    "            if i == 0 and conv_shortcut == True:\n",
    "                filters *= 2\n",
    "                x = basic_block(x, filters, conv_shortcut)\n",
    "            else:\n",
    "                x = basic_block(x, filters)\n",
    "        return x, filters\n",
    "    \n",
    "    inputs = keras.Input(shape=(resolution, resolution, 3))\n",
    "    filters = 64\n",
    "    '''\n",
    "    ## simple augmentation pipeline\n",
    "    simple_aug = keras.Sequential([\n",
    "        keras.layers.RandomFlip('horizontal'),\n",
    "        keras.layers.RandomRotation(factor=0.02),\n",
    "        keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2)\n",
    "    ])\n",
    "    x = simple_aug(inputs)\n",
    "    '''\n",
    "    ## random augmentation pipeline is included in data preprocessing\n",
    "    x = inputs\n",
    "    ## stem\n",
    "    if 'cifar' in dataset:\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters, 3, padding='same', use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=KR_VALUE\n",
    "        )(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "    elif dataset == 'imagenet':\n",
    "        x = keras.layers.Conv2D(\n",
    "            filters, 7, strides=2, padding='same', use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=KR_VALUE\n",
    "        )(x)\n",
    "        x = keras.layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5)(x)\n",
    "        x = keras.layers.Activation('relu')(x)\n",
    "        x = keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    ## trunk\n",
    "    for i, stack in enumerate(stack_list):\n",
    "        if i == 0:\n",
    "            x, filters = basic_stack(x, filters, stack)\n",
    "        else:\n",
    "            x, filters = basic_stack(x, filters, stack, True)\n",
    "    ## classifier\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    outputs = keras.layers.Dense(\n",
    "        classes, activation='softmax',\n",
    "        kernel_regularizer=KR_VALUE\n",
    "    )(x)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53337f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = load_imagenet(\n",
    "    resolution=resolution, batch_size=batch_size,\n",
    "    num_augment=num_augment, magnitude_augment=magnitude_augment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adffc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_resnet(dataset=dataset, depth=depth, dropout_rate=dropout_rate, resolution=resolution)\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr, milestones, gamma: float = 0.1):\n",
    "    if epoch in milestones:\n",
    "        lr *= gamma\n",
    "    return lr\n",
    "\n",
    "class TimeCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.history = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time_epoch_begin = time.perf_counter()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.history.append(time.perf_counter() - self.time_epoch_begin)\n",
    "\n",
    "lr_scheduler_callback = keras.callbacks.LearningRateScheduler(\n",
    "    lambda x, y: lr_schedule(x, y, milestones=milestones, gamma=gamma)\n",
    ")\n",
    "\n",
    "time_callback = TimeCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9773bd5-e1d0-45aa-b110-8ee02501fb56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=momentum,\n",
    "        #weight_decay=weight_decay\n",
    "        # `decay_steps` in `keras.optimizers.schedules.LearningRateSchedule`\n",
    "        # means batches instead of epochs, which is a fine grained value,\n",
    "        # so try to use `keras.callbacks.LearningRateScheduler`\n",
    "        # to set the learning rate decay value each epoch.\n",
    "    ),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    "    #jit_compile=JIT_COMPILE_FLAG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2cde0-014f-4d0d-8295-5f24d940d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = model.fit(\n",
    "    dataloader['train'],\n",
    "    epochs=epochs,\n",
    "    verbose='auto',\n",
    "    callbacks=[time_callback, lr_scheduler_callback],\n",
    "    validation_data=dataloader['val']\n",
    ")\n",
    "logs.history['t'] = time_callback.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5830196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
